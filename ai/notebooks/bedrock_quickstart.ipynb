{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8e1f9f6",
   "metadata": {},
   "source": [
    "# Amazon Bedrock Quickstart\n",
    "\n",
    "This notebook demonstrates basic usage of Amazon Bedrock for:\n",
    "- Listing available foundation models\n",
    "- Text generation with Claude\n",
    "- Embeddings with Titan\n",
    "- Using the Converse API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b61b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "# Set region\n",
    "os.environ[\"AWS_REGION\"] = \"us-east-1\"\n",
    "\n",
    "config = Config(\n",
    "    region_name=os.getenv(\"AWS_REGION\"),\n",
    "    retries={\"max_attempts\": 3, \"mode\": \"adaptive\"}\n",
    ")\n",
    "\n",
    "bedrock = boto3.client(\"bedrock\", config=config)\n",
    "bedrock_runtime = boto3.client(\"bedrock-runtime\", config=config)\n",
    "\n",
    "print(f\"✓ Bedrock clients initialized for region: {os.getenv('AWS_REGION')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989e1d94",
   "metadata": {},
   "source": [
    "## 1. List Available Foundation Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783aad73",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock.list_foundation_models()\n",
    "\n",
    "print(\"Available Foundation Models:\\n\")\n",
    "for model in response[\"modelSummaries\"][:10]:  # Show first 10\n",
    "    print(f\"• {model['modelId']}\")\n",
    "    print(f\"  Provider: {model['providerName']}\")\n",
    "    print(f\"  Modalities: {', '.join(model.get('inputModalities', []))}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94aba3d",
   "metadata": {},
   "source": [
    "## 2. Text Generation with Claude 3 Sonnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d416be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "prompt = \"Explain Amazon SageMaker in 2 sentences.\"\n",
    "\n",
    "body = json.dumps({\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "    \"max_tokens\": 1024,\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ]\n",
    "})\n",
    "\n",
    "response = bedrock_runtime.invoke_model(\n",
    "    modelId=model_id,\n",
    "    body=body\n",
    ")\n",
    "\n",
    "response_body = json.loads(response[\"body\"].read())\n",
    "text = response_body[\"content\"][0][\"text\"]\n",
    "\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "print(f\"Response:\\n{text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d879db49",
   "metadata": {},
   "source": [
    "## 3. Generate Embeddings with Titan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf455b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"amazon.titan-embed-text-v2:0\"\n",
    "text = \"Amazon Bedrock provides foundation models via API\"\n",
    "\n",
    "body = json.dumps({\n",
    "    \"inputText\": text\n",
    "})\n",
    "\n",
    "response = bedrock_runtime.invoke_model(\n",
    "    modelId=model_id,\n",
    "    body=body\n",
    ")\n",
    "\n",
    "response_body = json.loads(response[\"body\"].read())\n",
    "embeddings = response_body[\"embedding\"]\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Embedding dimensions: {len(embeddings)}\")\n",
    "print(f\"First 10 values: {embeddings[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec17237",
   "metadata": {},
   "source": [
    "## 4. Converse API - Unified Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8bf6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "response = bedrock_runtime.converse(\n",
    "    modelId=model_id,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": \"What are the benefits of serverless architecture? Be concise.\"}]\n",
    "        }\n",
    "    ],\n",
    "    inferenceConfig={\n",
    "        \"maxTokens\": 512,\n",
    "        \"temperature\": 0.7,\n",
    "        \"topP\": 0.9\n",
    "    }\n",
    ")\n",
    "\n",
    "assistant_message = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "print(f\"Response:\\n{assistant_message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eccc87",
   "metadata": {},
   "source": [
    "## 5. Multi-turn Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f2caf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = []\n",
    "\n",
    "# Turn 1\n",
    "conversation.append({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\"text\": \"What is AWS Lambda?\"}]\n",
    "})\n",
    "\n",
    "response = bedrock_runtime.converse(\n",
    "    modelId=model_id,\n",
    "    messages=conversation,\n",
    "    inferenceConfig={\"maxTokens\": 256, \"temperature\": 0.7}\n",
    ")\n",
    "\n",
    "assistant_msg_1 = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "conversation.append({\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": [{\"text\": assistant_msg_1}]\n",
    "})\n",
    "\n",
    "print(\"Turn 1\")\n",
    "print(f\"User: What is AWS Lambda?\")\n",
    "print(f\"Assistant: {assistant_msg_1}\\n\")\n",
    "\n",
    "# Turn 2\n",
    "conversation.append({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\"text\": \"How does it compare to EC2?\"}]\n",
    "})\n",
    "\n",
    "response = bedrock_runtime.converse(\n",
    "    modelId=model_id,\n",
    "    messages=conversation,\n",
    "    inferenceConfig={\"maxTokens\": 256, \"temperature\": 0.7}\n",
    ")\n",
    "\n",
    "assistant_msg_2 = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "print(\"Turn 2\")\n",
    "print(f\"User: How does it compare to EC2?\")\n",
    "print(f\"Assistant: {assistant_msg_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f17a893",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- Explore different models and parameters\n",
    "- Implement RAG (Retrieval Augmented Generation) with embeddings\n",
    "- Use LangChain for more complex workflows (see `langchain_bedrock_example.py`)\n",
    "- Check model access in the Bedrock console if you encounter permission errors"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
